{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.chunk import RegexpParser\n",
    "import nertclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PART 0:\n",
    "##Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We abtained the source data from (TODO Jordan)\n",
    "\n",
    "We first eliminated TV shows and focused on only moviews.\n",
    "\n",
    "Then we reduced the movies to ones with 10 or more IMDB reviews so that we only analyze movies with decent infleunce.\n",
    "\n",
    "We then matched the IMDB movie titles with their wikipedia counterparts, and this turned out to be a challenge on its own. Since our IMDB dataset is larger then the wikipedia dataset, many movies have IMDB summaries but not a wikipedia summary. Our final database included movies that have IMDB summaries and/or wikipedia summaries.\n",
    "\n",
    "Lastly, we matched our movies to their country data, and reduced our dataset to just US movies. It contains about 34,000 movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PART 1: \n",
    "##Named Entity Recognition Tagging for Movie Summaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import Standford POS Tagger and Stanford NER Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to run StanfordPostTagger and NERTagger, \n",
    "#first download these two packages from http://nlp.stanford.edu/software/CRF-NER.shtml \n",
    "#I saved the downloaded files in lib/\n",
    "post = StanfordPOSTagger('lib/stanford-postagger-2014-08-27/models/english-bidirectional-distsim.tagger', \n",
    "                         'lib/stanford-postagger-2014-08-27/stanford-postagger.jar', 'utf-8') # doctest: +SKIP\n",
    "# post.tag('What is the airspeed of an unladen swallow ?'.split()) # doctest: +SKIP\n",
    "nert = StanfordNERTagger('lib/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                         'lib/stanford-ner-2014-08-27/stanford-ner.jar', 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load in cleaned dataset of USA moviews from IMDB and Wikiperida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('usamoviedict.array.json') as sample:\n",
    "# with open('movies-with-roles-summaries.json') as sample:\n",
    "    sample = json.load(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###More data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of ([[ in the wikipidia summaries\n",
    "for movie in sample:\n",
    "    summaries_wiki = movie[1]['summaries_wikipedia']\n",
    "    if len(summaries_wiki) > 0:\n",
    "        summaries_wiki = re.sub(r'\\s\\(\\[\\[', ' ', summaries_wiki[0])\n",
    "        movie[1]['summaries_wikipedia'][0] = summaries_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###NER Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pos tagging using nltk.pos_tag\n",
    "#ner tagging using stanford ner tagger\n",
    "def ie_preprocess(document, lower='false', stage=\"pos\"):\n",
    "#     if stage == 'pos':\n",
    "#         sentences = nltk.sent_tokenize(document)\n",
    "#         sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "#         if lower == 'false':\n",
    "#             for sent in sentences:\n",
    "#                 for i in range(len(sent)):\n",
    "#                     if sent[i] != sent[i].lower():\n",
    "#                         sent[i] = sent[i].lower()\n",
    "#         sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "#         return sentences\n",
    "    if stage == \"ner\":\n",
    "        return nertclient.nertclient(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make stanford nert more accurate by seperating the last word\n",
    "# in a sentence from the ending punctuation with a space.\n",
    "def insert_space_before_punct(s):\n",
    "    # return re.sub(r'([-\\/#!$%\\^&\\*;:{}=\\-_`~()])', r' \\1', s)\n",
    "    # s = re.sub(r'([-\\/#$%\\^&\\*{}=\\-_`~()])', r'', s)\n",
    "    s = re.sub(r'([.,!?;:])', r' \\1', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pos + ner tag summaries and roles\n",
    "for movie in sample:\n",
    "    summaries_imdb = movie[1][\"summaries_imdb\"]\n",
    "    summaries_wiki = movie[1]['summaries_wikipedia']\n",
    "    roles = movie[1]['roles']\n",
    "    if len(summaries_imdb) > 0:\n",
    "        # sample[i][1][\"summaries_imdb_pos\"] = ie_preprocess(summaries_imdb[0])\n",
    "        sample[i][1][\"summaries_imdb_ner\"] = ie_preprocess(insert_space_before_punct(summaries_imdb[0]), stage=\"ner\")\n",
    "        \n",
    "    if len(summaries_wiki) > 0:\n",
    "        # sample[i][1][\"summaries_wikipedia_pos\"] = ie_preprocess(summaries_wiki[0])\n",
    "        sample[i][1][\"summaries_wikipedia_ner\"] = ie_preprocess(insert_space_before_punct(summaries_wiki[0]), stage='ner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Output NER tagged dataset as json file usamoviedict.sample.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_default(obj):\n",
    "    if isinstance(obj, set):\n",
    "        d = {}\n",
    "        for i in obj:\n",
    "            d[i] = 1\n",
    "        return d\n",
    "        # return list(obj)\n",
    "    raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(filename + '.sample.json', 'w') as outfile:\n",
    "    json.dump(sample, outfile, ensure_ascii=False, default=set_default)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PART 2: \n",
    "##Lift out character names from summaries, and associate the character names with names from the IMDB role list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load in the previously NER tagged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('usamoviedict.array.json.0.sample.json') as tagged_data:\n",
    "    sample = json.load(tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create a new master dictionary called \"sum_and_char\" containing character names and related info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####First, add new fields \"summaries_combined\" and \"summaries_combined_ner\" in the dataset that stores wikipedia and IMDB summaries into a single string for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine wiki_ner and imdb_nerie\n",
    "for movie in sample:\n",
    "    combined = []\n",
    "    combined_ner = \"\"\n",
    "    if 'summaries_wikipedia_ner' in movie[1]:\n",
    "        combined.extend(movie[1]['summaries_wikipedia'] )\n",
    "        combined_ner = combined_ner + \" \" + (movie[1]['summaries_wikipedia_ner'] )\n",
    "    if 'summaries_imdb_ner' in movie[1]:\n",
    "        combined.extend(movie[1][\"summaries_imdb\"])\n",
    "        combined_ner = combined_ner + \" \" + (movie[1][\"summaries_imdb_ner\"])\n",
    "    movie[1]['summaries_combined'] = combined\n",
    "    movie[1]['summaries_combined_ner'] = combined_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Second, run a customized regex chuncker to lift out tagged names from combined summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a json file containing a list of names for the social security dataset, this file will be used in the person_extractor_and_replacer function. The resulting name_list.json contains 138,036 unique names (93,889 first names, 58,258 last names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('names.json') as data:\n",
    "    first_names = json.load(data)\n",
    "with open('surnames.json') as data2:\n",
    "    last_names = json.load(data2)\n",
    "\n",
    "name_list = []\n",
    "\n",
    "for key, value in first_names.items():\n",
    "    name_list.append(key)\n",
    "for key, value in last_names.items():\n",
    "    name_list.append(key)\n",
    "\n",
    "def set_default(obj):\n",
    "    if isinstance(obj, set):\n",
    "        d = {}\n",
    "        for i in obj:\n",
    "            d[i] = 1\n",
    "        return d\n",
    "    raise TypeError\n",
    "                    \n",
    "with open(\"name_list.json\", 'w') as outfile:\n",
    "    json.dump(set(name_list), outfile, ensure_ascii=False, default=set_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#character name chunking.\n",
    "#1. tag words in summaries that match the Social Security name dataset as \"/PERSON\"\n",
    "#2. chunk together the consective \"/PERSON\" tagged words\n",
    "#3. append to a chunk any number of capitalized word after a '/PERSON\" tagged word (this helps capture the entirity\n",
    "# of a name that spans multiple words. some of the words toward the end may not have been tagged correctly.)\n",
    "\n",
    "def person_extractor_and_replacer(tagged_summary):\n",
    "    with open('name_list.json', 'r') as json_str:\n",
    "        ssn_names = json.load(json_str)\n",
    "\n",
    "    words = tagged_summary.split()\n",
    "\n",
    "    persons = []\n",
    "    current_person = []\n",
    "\n",
    "    for w in words:\n",
    "        m = re.search('([^/]+)/([A-Z]+)', w)\n",
    "        if m is not None:\n",
    "            body = m.group(1)\n",
    "            klass = m.group(2)\n",
    "            if klass != \"PERSON\":\n",
    "                if body in ssn_names:\n",
    "                    klass = \"PERSON\"\n",
    "\n",
    "            if klass == \"PERSON\":\n",
    "                current_person.append(body)\n",
    "                 #print(body, klass)\n",
    "            elif len(current_person) > 0 and re.search('^[A-Z]', w) is not None:\n",
    "                current_person.append(body)\n",
    "            elif len(current_person) > 0:\n",
    "                persons.append(\" \".join(current_person))\n",
    "                # persons.append(current_person)\n",
    "                current_person = []\n",
    "            # search for matches with social security name list\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating a new dict called sum_and_char to store summaries and corresponding characters\n",
    "sum_and_char = {}\n",
    "for movie in sample:\n",
    "#     sorting entity_chuncking results into summaries and characters\n",
    "    result = person_extractor_and_replacer(movie[1]['summaries_combined_ner'])\n",
    "    movie_name = movie[0]\n",
    "    if len(result) <= 0:\n",
    "        char_data = {}\n",
    "    else:\n",
    "        char_data = set(result)\n",
    "    sum_and_char[movie_name] = {\"sum\": movie[1]['summaries_combined'][0], \\\n",
    "                                \"char_raw\": char_data,\\\n",
    "                                \"char_info\" : {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Filter character names, and establish the link between filtered character names from summaries and their counter parts in the roles list. Store results in the field char_info dict within the master dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for movie in sample:\n",
    "#     making a bag of words for roles\n",
    "    for role in movie[1]['roles']:\n",
    "        role_words = role['role']\n",
    "        role_bag = re.split(r' |/|\\'|\\\"', role_words)\n",
    "        role['role_bag'] = (set(role_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter char_raw against role list and link char_names with their names in the role list.\n",
    "''' \n",
    "match words in char_raw against roles in role list. the list of actual character names is a subset of char_raw.\n",
    "This is because some of the captured names are not characters (e.g. director, actor, spriris and saints). We filter\n",
    "out the false names by only keeping char_raw names if at least one word in the name has a matched word in the role list.\n",
    "If there is zero word match, discard. If there is at least one\n",
    "match, keep.\n",
    "\n",
    "char_info in sum_and_char use names in the role list as keys, and contains role gender, char names\n",
    "found in summaries, and roles found in summaries\n",
    "\n",
    "The matching selects the highest # of matches between a role and a character name. Ties are broken by selecting the role with\n",
    "the fewest words.\n",
    "'''\n",
    "for key, value in (sum_and_char.items()):\n",
    "    for movie in sample:\n",
    "#     locate correct movie\n",
    "        if movie[0] == key:\n",
    "#             print()\n",
    "#             print(\"--------\", key, \"--------------------------------------\")\n",
    "#             for each filted char name\n",
    "            for name in value['char_raw']:\n",
    "                max_count = 0\n",
    "                max_role = None\n",
    "                max_gen = None\n",
    "#                 tie-breaker on # of words contained in a role_bag\n",
    "                max_role_bag_len = 0\n",
    "\n",
    "#                 tied_list = []\n",
    "                name_split = name.split()\n",
    "#                 for each role\n",
    "                for role in movie[1]['roles']:\n",
    "                    count = 0\n",
    "#                   for each word in char name\n",
    "#                   increment count by 1 every time a word in the char name appears in a role\n",
    "                    for word in name_split:\n",
    "                        if word in role['role_bag']:\n",
    "                            count += 1\n",
    "#                   select the role with the most counts!\n",
    "                    if (count > max_count) or (count == max_count and len(role['role_bag']) < max_role_bag_len):\n",
    "                        max_count = count\n",
    "                        max_role = role['role']\n",
    "                        max_gen = role['gender']\n",
    "                        max_role_bag_len = len(role['role_bag'])\n",
    "                \n",
    "#                 After looping through all roles in the role list, decide which is the max role\n",
    "                if max_count < 1:\n",
    "#                     print(name, \"discarded\")\n",
    "                    pass\n",
    "                else:   \n",
    "                    if max_role not in value['char_info']:\n",
    "                        value['char_info'][max_role] = {'gender':max_gen, 'roles_found_in_sums':[], 'names_found_in_sums':[name]}\n",
    "                    else:\n",
    "                        value['char_info'][max_role]['names_found_in_sums'].append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lastly, output the master dict sum_and_char as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('sam_char_char.complete.v1.json', 'w') as outfile:\n",
    "    json.dump(sum_and_char, outfile, ensure_ascii=False, default=set_default)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PART 3: \n",
    "###Find roles for character names in summary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PART 4:\n",
    "###Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

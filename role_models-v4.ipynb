{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.chunk import RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import Standford POS Tagger and Stanford NER Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to run StanfordPostTagger and NERTagger, \n",
    "#first download these two packages from http://nlp.stanford.edu/software/CRF-NER.shtml \n",
    "#I saved the downloaded files in lib/\n",
    "post = StanfordPOSTagger('lib/stanford-postagger-2014-08-27/models/english-bidirectional-distsim.tagger', \n",
    "                         'lib/stanford-postagger-2014-08-27/stanford-postagger.jar', 'utf-8') # doctest: +SKIP\n",
    "# post.tag('What is the airspeed of an unladen swallow ?'.split()) # doctest: +SKIP\n",
    "nert = StanfordNERTagger('lib/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                         'lib/stanford-ner-2014-08-27/stanford-ner.jar', 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a random sample of 2 and smaple of 5 from the json data as starting data.\n",
    "Note: only run this code once. Currently commented out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('movies-with-roles-summaries.json')as data_file:\n",
    "#     data = json.load(data_file)\n",
    "\n",
    "# print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_sample_2 = random.sample((data.items()), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_sample_5 = random.sample((data.items()), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('data_sample_2.json', 'w') as outfile:\n",
    "#     json.dump(data_sample_2, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('data_sample_5.json', 'w') as outfile:\n",
    "#     json.dump(data_sample_5, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load in the sample of five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data_sample_5.json') as sample:\n",
    "# with open('movies-with-roles-summaries.json') as sample:\n",
    "    sample = json.load(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###More data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of ([[ in the wikipidia summaries\n",
    "for movie in sample:\n",
    "    summaries_wiki = movie[1]['summaries_wikipedia']\n",
    "    if len(summaries_wiki) > 0:\n",
    "        summaries_wiki = re.sub(r'\\s\\(\\[\\[', ' ', summaries_wiki[0])\n",
    "        movie[1]['summaries_wikipedia'][0] = summaries_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pos tagging using nltk.pos_tag\n",
    "#ner tagging using stanford ner tagger\n",
    "def ie_preprocess(document, lower='false', stage=\"pos\"):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    if lower == 'false':\n",
    "        for sent in sentences:\n",
    "            for i in range(len(sent)):\n",
    "                if sent[i] != sent[i].lower():\n",
    "                    sent[i] = sent[i].lower()\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    if stage == \"pos\":\n",
    "        return sentences\n",
    "    if stage == \"ner\":\n",
    "        return nert.tag(document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pos + ner tag summaries and roles\n",
    "for movie in sample:\n",
    "    summaries_imdb = movie[1][\"summaries_imdb\"]\n",
    "    summaries_wiki = movie[1]['summaries_wikipedia']\n",
    "    roles = movie[1]['roles']\n",
    "    if len(summaries_imdb) > 0:\n",
    "        movie[1][\"summaries_imdb_pos\"] = ie_preprocess(summaries_imdb[0])\n",
    "        movie[1][\"summaries_imdb_ner\"] = ie_preprocess(summaries_imdb[0], stage=\"ner\")\n",
    "    if len(summaries_wiki) > 0:\n",
    "        movie[1][\"summaries_wikipedia_pos\"] = ie_preprocess(summaries_wiki[0])\n",
    "        movie[1][\"summaries_wikipedia_ner\"] = ie_preprocess(summaries_wiki[0], stage='ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Search for a ner tagged name in the roles bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####First, combine wiki summaries and imdb summaries into one string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine wiki_ner and imdb_ner\n",
    "for movie in sample:\n",
    "#     summaries_imdb_ner is indeed a list\n",
    "    summaries_imdb_ner = movie[1][\"summaries_imdb_ner\"]\n",
    "    if 'summaries_wikipedia_ner' in movie[1]:\n",
    "        summaries_wiki_ner = movie[1]['summaries_wikipedia_ner']\n",
    "    else: summaries_wiki_ner = []\n",
    "    summaries_imdb_ner.extend(summaries_wiki_ner)\n",
    "    movie[1]['summaries_combined_ner'] = summaries_imdb_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Run chuncker to lift out tagged names from summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define custom tagged entities - group NE's together \n",
    "def chunker_rules(values):\n",
    "    # Define  custom grammar (modified to be a valid regex).\n",
    "    grammar = r'''\n",
    "        PERSON:\n",
    "                {<PERSON>+}\n",
    "            '''\n",
    "    cp = nltk.RegexpParser(grammar) # Create an instance of your custom parser.\n",
    "    return cp.parse(values)         # Parse!\n",
    "\n",
    "def entity_chunker(tagged_docs):\n",
    "    chunks = []\n",
    "#     for doc in tagged_docs:\n",
    "    tree = chunker_rules(tagged_docs)\n",
    "    for subtree in tree.subtrees():\n",
    "#             if (subtree.node == 'WIDOW'):\n",
    "        leaflist = [leaf[0] for leaf in subtree.leaves()]\n",
    "        chunks.append(' '.join(leaflist))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating a new dict called sum_and_char to store summaries and corresponding characters\n",
    "sum_and_char = {}\n",
    "for movie in sample:\n",
    "#     making a bag of words for roles\n",
    "    roles_bag = []\n",
    "    roles = movie[1]['roles']\n",
    "    for role in roles:\n",
    "        roles_bag.append(role['role'])\n",
    "    str = \" \".join(roles_bag)\n",
    "#     split roles on space, /, ' or \"\n",
    "    roles_bag = re.split(r' |/|\\'|\\\"', str)\n",
    "#     sorting entity_chuncking results into summaries and characters\n",
    "    result = entity_chunker(movie[1]['summaries_combined_ner'])\n",
    "    movie_name = movie[0]\n",
    "    if len(result) <= 1:\n",
    "        char_data = {}\n",
    "    else:\n",
    "        char_data = set(result[1:])\n",
    "    sum_and_char[movie_name] = {\"sum\": result[0], \\\n",
    "                                \"char_raw\": char_data,\\\n",
    "                                \"roles_bag\": set(roles_bag),\\\n",
    "                                \"char_info\" : {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Check characters against roles for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in sum_and_char.items():\n",
    "    sum_and_char[key]['char_filtered'] = []\n",
    "    for char in value['char_raw']:\n",
    "        flag = False\n",
    "        char_split = char.split()\n",
    "        for elem in char_split:\n",
    "            if elem in value['roles_bag']:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            sum_and_char[key]['char_filtered'].append(char)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Establish the link between filtered character names from summaries and their counter parts in the roles list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for movie in sample:\n",
    "#     making a bag of words for roles\n",
    "    for role in movie[1]['roles']:\n",
    "        role_words = role['role']\n",
    "        role_bag = re.split(r' |/|\\'|\\\"', role_words)\n",
    "        role['role_bag'] = (set(role_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: add algorithm for tie-breaking\n",
    "\n",
    "# link char_filter with their names in the role list.\n",
    "''' char_info in sum_and_char use names in the role list as keys, and contains role gender, char names\n",
    "found in summaries, and roles found in summaries\n",
    "'''\n",
    "for key, value in (sum_and_char.items()):\n",
    "    for movie in sample:\n",
    "#     locate correct movie\n",
    "        if movie[0] == key:\n",
    "#             for each filted char name\n",
    "            for name in value['char_filtered']:\n",
    "                max_count = 0\n",
    "                max_role = None\n",
    "                max_gen = None\n",
    "                name_split = name.split()\n",
    "#                 for each role\n",
    "                for role in movie[1]['roles']:\n",
    "                    count = 0\n",
    "#                   for each word in char name\n",
    "#                   increment count by 1 every time a word in the char name appears in a role\n",
    "                    for word in name_split:\n",
    "                        if word in role['role_bag']:\n",
    "                            count += 1\n",
    "#                   select the role with the most counts!\n",
    "                    if count > max_count:\n",
    "                        max_count = count\n",
    "                        max_role = role['role']\n",
    "                        max_gen = role['gender']\n",
    "                if max_count < 1:\n",
    "                    print(\"error\")\n",
    "                    pass\n",
    "                else:\n",
    "                    if max_role not in value['char_info']:\n",
    "                        value['char_info'][max_role] = {'gender':max_gen, 'roles_found_in_sums':[], 'names_found_in_sums':[name]}\n",
    "                    else:\n",
    "                        value['char_info'][max_role]['names_found_in_sums'].append(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
